{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import xticks\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "#from wordcloud import WordCloud,STOPWORDS\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= pd.read_csv(r'C:\\Users\\vikil\\Desktop\\ML\\data_Sets\\twitter\\train.csv')\n",
    "test=pd.read_csv(r'C:\\Users\\vikil\\Desktop\\ML\\data_Sets\\twitter\\test.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 5), (3263, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location    2533\n",
       "keyword       61\n",
       "target         0\n",
       "text           0\n",
       "id             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of real disaster tweets 3271\n",
      "No. of fake disaster tweets 4342\n"
     ]
    }
   ],
   "source": [
    "print('No. of real disaster tweets' , len(train[train['target']==1]))\n",
    "print('No. of fake disaster tweets' ,len(train[train['target']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_real=len(train[train['target']==1])\n",
    "train_fake=len(train[train['target']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000029B7A346788>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "real=[]\n",
    "fake=[]\n",
    "for i in range(0,len(train['text'])):\n",
    "    if (train.iloc[i,4])==1:\n",
    "        real.append(train['text'])\n",
    "    else:\n",
    "        fake.append(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3271"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_barplot(category,length,xlabel,ylabel,color):\n",
    "    plt.bar(category,height=length,color='mykw')\n",
    "    plt.legend()\n",
    "    plt.xlabel=(xlabel)\n",
    "    plt.ylabel=(ylabel)\n",
    "    plt.title('Number of real and fake tweets')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_barplot(['REAL','FAKE'],[train_real,train_fake],'Type of Tweet','Number of Tweets',color=['m','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(text):    \n",
    "    return len(text)\n",
    "\n",
    "train[\"length\"]= train.text.apply(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.03743596479706"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0     95.706817\n",
       "1    108.113421\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('target').mean()['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = list(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=[]\n",
    "for sentence in train.text:\n",
    "    for word in sentence.split():\n",
    "        if word in stop:\n",
    "            sw.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = nltk.FreqDist(sw)\n",
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = wordlist.most_common(10)\n",
    "len(top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2575), ('a', 1845), ('to', 1805), ('in', 1757), ('of', 1722), ('and', 1302), ('for', 820), ('is', 814), ('on', 773), ('you', 632)]\n"
     ]
    }
   ],
   "source": [
    "print(top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[]\n",
    "for sentence in train.text:\n",
    "    for pun in sentence:\n",
    "        if pun in punctuation:\n",
    "            p.append(pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "punlist=nltk.FreqDist(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_pun=punlist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/', 14585),\n",
       " ('.', 11696),\n",
       " (':', 6910),\n",
       " ('#', 3403),\n",
       " (\"'\", 3157),\n",
       " ('?', 3126),\n",
       " ('@', 2759),\n",
       " ('-', 1753),\n",
       " ('!', 1173),\n",
       " ('_', 863)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_pun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_pun = stop + punctuation\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the messages\n",
    "def preprocess(tweet):\n",
    "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet) # removing urls \n",
    "    tweet = re.sub('[^\\w]',' ',tweet) # remove embedded special characters in words (for example #earthquake)         \n",
    "    tweet = re.sub('[\\d]','',tweet) # this will remove numeric characters\n",
    "    tweet = tweet.lower()\n",
    "    words = tweet.split()  \n",
    "    sentence = \"\"\n",
    "    for word in words:     \n",
    "        if word not in (sw_pun):  # removing stopwords & punctuations                \n",
    "            word = lemma.lemmatize(word,pos = 'v')  # converting to lemma    \n",
    "            if len(word) > 3: # we will consider words with length  greater than 3 only\n",
    "                sentence = sentence + word + ' '             \n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda s : preprocess(s))\n",
    "test['text'] = test['text'].apply(lambda s : preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda s : remove_emoji(s))\n",
    "test ['text'] = test ['text'].apply(lambda s : remove_emoji(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def create_vocab(df):\n",
    "    vocab = Counter()\n",
    "    for i in range(df.shape[0]):\n",
    "        vocab.update(df.text[i].split())\n",
    "    return(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "master=pd.concat((train,test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16439"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=nltk.FreqDist(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 560),\n",
       " ('fire', 534),\n",
       " ('bomb', 338),\n",
       " ('news', 299),\n",
       " ('people', 286),\n",
       " ('burn', 262),\n",
       " ('time', 254),\n",
       " ('kill', 253),\n",
       " ('make', 248),\n",
       " ('attack', 240),\n",
       " ('flood', 233),\n",
       " ('crash', 232),\n",
       " ('build', 231),\n",
       " ('emergency', 229),\n",
       " ('video', 228),\n",
       " ('come', 223),\n",
       " ('disaster', 220),\n",
       " ('take', 217),\n",
       " ('would', 214),\n",
       " ('body', 209),\n",
       " ('think', 204),\n",
       " ('police', 199),\n",
       " ('look', 193),\n",
       " ('know', 192),\n",
       " ('love', 190),\n",
       " ('watch', 188),\n",
       " ('home', 187),\n",
       " ('storm', 187),\n",
       " ('still', 181),\n",
       " ('train', 177),\n",
       " ('suicide', 177),\n",
       " ('live', 172),\n",
       " ('first', 170),\n",
       " ('collapse', 169),\n",
       " ('wind', 165),\n",
       " ('back', 164),\n",
       " ('scream', 164),\n",
       " ('california', 159),\n",
       " ('want', 156),\n",
       " ('drown', 152),\n",
       " ('cause', 151),\n",
       " ('need', 150),\n",
       " ('work', 149),\n",
       " ('today', 149),\n",
       " ('world', 148),\n",
       " ('nuclear', 148),\n",
       " ('hiroshima', 147),\n",
       " ('year', 143),\n",
       " ('full', 143),\n",
       " ('service', 142)]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab = []\n",
    "min_occur = 2\n",
    "for x,y in vocab.items():\n",
    "    if y >= min_occur:\n",
    "        final_vocab.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6024"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fgh \n"
     ]
    }
   ],
   "source": [
    "sentence=\"\"\n",
    "word='fgh'\n",
    "print(sentence + word + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(tweet):\n",
    "    sentence = \"\"\n",
    "    for word in tweet.split():\n",
    "        if word in final_vocab:\n",
    "            sentence=sentence + word+' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text']=train['text'].apply(lambda s : cleaner(s))\n",
    "test ['text'] = test ['text'].apply(lambda s : cleaner(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = train[train.target==1].reset_index()\n",
    "fake = train[train.target==0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(data,n):\n",
    "    all_words=[]\n",
    "    for i in range(len(data)):\n",
    "        temp = data[\"text\"][i].split()\n",
    "        for word in temp:\n",
    "            all_words.append(word)\n",
    "            \n",
    "    tokenized = all_words\n",
    "    esBigrams = ngrams(tokenized, n)\n",
    "    \n",
    "    esBigram_wordlist = nltk.FreqDist(esBigrams)\n",
    "    top100 = esBigram_wordlist.most_common(100)\n",
    "    top100 = dict(top100)\n",
    "    df_ngrams = pd.DataFrame(sorted(top100.items(), key=lambda x: x[1])[::-1])\n",
    "    return df_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_unigrams = get_ngrams(real,1)\n",
    "fake_unigrams = get_ngrams(fake,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(fire,)</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(bomb,)</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(kill,)</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(news,)</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(flood,)</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>(evacuation,)</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(saudi,)</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(rain,)</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(could,)</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(update,)</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0    1\n",
       "0         (fire,)  273\n",
       "1         (bomb,)  187\n",
       "2         (kill,)  161\n",
       "3         (news,)  145\n",
       "4        (flood,)  122\n",
       "..            ...  ...\n",
       "95  (evacuation,)   40\n",
       "96       (saudi,)   39\n",
       "97        (rain,)   39\n",
       "98       (could,)   38\n",
       "99      (update,)   37\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\vikil\\Anaconda3\\ana\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vikil\\Anaconda3\\ana\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vikil\\Anaconda3\\ana\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vikil\\Anaconda3\\ana\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vikil\\Anaconda3\\ana\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vikil\\Anaconda3\\ana\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.text\n",
    "y = train.target\n",
    "\n",
    "test_id = test.id\n",
    "test.drop([\"id\",\"location\",\"keyword\"],1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(X_train)\n",
    "X_train_set = tokenizer.texts_to_matrix(X_train, mode = 'freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [get_f1])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    " #   plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               705920    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 706,049\n",
      "Trainable params: 706,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_words = X_train_set.shape[1]\n",
    "model = define_model(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vikil\\Anaconda3\\ana\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " - 27s - loss: 0.6302 - get_f1: 0.3357\n",
      "Epoch 2/10\n",
      " - 12s - loss: 0.4432 - get_f1: 0.7678\n",
      "Epoch 3/10\n",
      " - 12s - loss: 0.3447 - get_f1: 0.8260\n",
      "Epoch 4/10\n",
      " - 12s - loss: 0.2878 - get_f1: 0.8551\n",
      "Epoch 5/10\n",
      " - 14s - loss: 0.2475 - get_f1: 0.8830\n",
      "Epoch 6/10\n",
      " - 12s - loss: 0.2167 - get_f1: 0.8965\n",
      "Epoch 7/10\n",
      " - 12s - loss: 0.1917 - get_f1: 0.9076\n",
      "Epoch 8/10\n",
      " - 12s - loss: 0.1721 - get_f1: 0.9223\n",
      "Epoch 9/10\n",
      " - 12s - loss: 0.1547 - get_f1: 0.9291\n",
      "Epoch 10/10\n",
      " - 11s - loss: 0.1401 - get_f1: 0.9381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x29b0b31e3c8>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit network\n",
    "model.fit(X_train_set,y_train,epochs=10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenizer on the test dataset\n",
    "test_set = tokenizer.texts_to_matrix(test.text, mode = 'freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets prepare for the prediction submission\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_id\n",
    "sub['target'] = y_test_pred\n",
    "sub.to_csv(r'C:\\Users\\vikil\\Desktop\\ML\\data_Sets\\twitter\\submission_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
